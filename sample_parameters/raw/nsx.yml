enable_ansible_debug: false # set value to true for verbose output from ansible

# format: "http://<jumphost_ip>:40001"
nsx_image_webserver: "http://192.168.110.11:40001"
#ova_file_name: "nsx-unified-appliance-2.2.0.0.0.8680778.ova" #Uncomment this if downloaded file manually and placed under /home/concourse
#ovftool_file_name: "VMware-ovftool-4.2.0-5965791-lin.x86_64.bundle"   #Uncomment this if downloaded file manually and placed under /home/concourse

# vCenter to deploy the NSX manager
vcenter_ip: 192.168.110.22
vcenter_username: administrator@corp.local
vcenter_password: "VMware1!"
vcenter_datacenter: RegionA01
vcenter_cluster: RegionA01-MGMT      #management cluster
vcenter_datastore: iscsi

# NSX manager general network settings
mgmt_portgroup: 'ESXi-RegionA01-vDS-COMP'
dns_server: 192.168.110.10
dns_domain: corp.local.io
ntp_servers: time.vmware.com
default_gateway: 192.168.110.1
netmask: 255.255.255.0

nsx_manager_ip: 192.168.110.33
nsx_manager_username: admin
nsx_manager_password: Admin!23Admin
nsx_manager_assigned_hostname: "nsxt-manager-10" # this hostname+dns_domain will be FQDN
nsx_manager_root_pwd: VMware1!    # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small   # Recommended for real barebones demo, smallest setup
nsx_manager_ssh_enabled: true
resource_reservation_off: true

# Compute manager credentials should be the same as above vCenter's if
# controllers and edges are to be on the same vCenter
compute_manager_username: "Administrator@vsphere.local"
compute_manager_password: "VMware1!"
# compute manager for the compute cluster (2nd vCenter)
compute_manager_2_vcenter_ip: "null"
compute_manager_2_username: "null"
compute_manager_2_password: "null"

edge_uplink_profile_vlan: 0 # For outbound uplink connection used by Edge, usually keep as 0
esxi_uplink_profile_vlan: 0 # For internal overlay connection used by Esxi hosts, usually trasnport VLAN ID

# Virtual Tunnel Endpoint network ip pool
vtep_ip_pool_cidr: 192.168.213.0/24
vtep_ip_pool_gateway: 192.168.213.1
vtep_ip_pool_start: 192.168.213.10
vtep_ip_pool_end: 192.168.213.200

# Tier 0 router
tier0_router_name: DefaultT0Router
tier0_uplink_port_ip: 192.168.100.4
tier0_uplink_port_subnet: 24
tier0_uplink_next_hop_ip: 192.168.100.1
tier0_uplink_port_ip_2: 192.168.100.5
tier0_ha_vip: 192.168.100.3

## Controllers
controller_ips: 192.168.110.34 #comma separated based on number of required controllers
controller_default_gateway: 192.168.110.1
controller_ip_prefix_length: 24
controller_hostname_prefix: controller # Generated hostname: controller_1.corp.local.io
controller_cli_password: "VMware1!" # Min 8 chars, upper, lower, num, special char
controller_root_password: "VMware1!"
controller_deployment_size: "SMALL"
vc_datacenter_for_controller: RegionA01
vc_cluster_for_controller: RegionA01-MGMT
vc_datastore_for_controller: iscsi
vc_management_network_for_controller: "ESXi-RegionA01-vDS-COMP"
controller_shared_secret: "VMware1!VMware1!"

## Edge nodes
edge_ips: 192.168.110.37,192.168.110.38    #comma separated based in number of required edges
edge_default_gateway: 192.168.110.1
edge_ip_prefix_length: 24
edge_hostname_prefix: nsx-t-edge
edge_transport_node_prefix: edge-transp-node
edge_cli_password: "VMware1!"
edge_root_password: "VMware1!"
edge_deployment_size: "large" #Large recommended for PKS deployments
vc_datacenter_for_edge: RegionA01
vc_cluster_for_edge: RegionA01-MGMT
vc_datastore_for_edge: iscsi
vc_uplink_network_for_edge: "ESXi-RegionA01-vDS-COMP"
vc_overlay_network_for_edge: "VM-RegionA01-vDS-COMP"
vc_management_network_for_edge: "ESXi-RegionA01-vDS-COMP"

## ESX hosts
#Intsll vSphere clusters automatically
clusters_to_install_nsx: RegionA01-MGMT,RegionA01-K8s    #Comma seprated
per_cluster_vlans: 0,0  #Comma seprated, order of VLANs applied same as order of clusters

esx_ips: "" # additional esx hosts, if any, to be individually installed
esx_os_version: "6.5.0"
esx_root_password: "ca$hc0w"
esx_hostname_prefix: "esx-host"

esx_available_vmnic: "vmnic1" # comma separated physical NICs, applies to both cluster installation or ESXi installation

nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  - name: T1-Router-PKS-Infra
    switches:
    - name: PKS-Infra
      logical_switch_gw: 192.168.50.1 # Last octet should be 1 rather than 0
      subnet_mask: 24
  - name: T1Router-PKS-Services
    switches:
    - name: PKS-Services
      logical_switch_gw: 192.168.60.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

